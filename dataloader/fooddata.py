import random
import torch
from torchvision import transforms
from PIL import Image
from transformers import BertTokenizer
from torch.utils.data import Dataset
from os.path import join
import json

# í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì´ 101ê°œ ìŒì‹ í´ë˜ìŠ¤)
CLASS_NAME = [
    "apple_pie",
    "baby_back_ribs",
    "baklava",
    "beef_carpaccio",
    "beef_tartare",
    "beet_salad",
    "beignets",
    "bibimbap",
    "bread_pudding",
    "breakfast_burrito",
    "bruschetta",
    "caesar_salad",
    "cannoli",
    "caprese_salad",
    "carrot_cake",
    "ceviche",
    "cheese_plate",
    "cheesecake",
    "chicken_curry",
    "chicken_quesadilla",
    "chicken_wings",
    "chocolate_cake",
    "chocolate_mousse",
    "churros",
    "clam_chowder",
    "club_sandwich",
    "crab_cakes",
    "creme_brulee",
    "croque_madame",
    "cup_cakes",
    "deviled_eggs",
    "donuts",
    "dumplings",
    "edamame",
    "eggs_benedict",
    "escargots",
    "falafel",
    "filet_mignon",
    "fish_and_chips",
    "foie_gras",
    "french_fries",
    "french_onion_soup",
    "french_toast",
    "fried_calamari",
    "fried_rice",
    "frozen_yogurt",
    "garlic_bread",
    "gnocchi",
    "greek_salad",
    "grilled_cheese_sandwich",
    "grilled_salmon",
    "guacamole",
    "gyoza",
    "hamburger",
    "hot_and_sour_soup",
    "hot_dog",
    "huevos_rancheros",
    "hummus",
    "ice_cream",
    "lasagna",
    "lobster_bisque",
    "lobster_roll_sandwich",
    "macaroni_and_cheese",
    "macarons",
    "miso_soup",
    "mussels",
    "nachos",
    "omelette",
    "onion_rings",
    "oysters",
    "pad_thai",
    "paella",
    "pancakes",
    "panna_cotta",
    "peking_duck",
    "pho",
    "pizza",
    "pork_chop",
    "poutine",
    "prime_rib",
    "pulled_pork_sandwich",
    "ramen",
    "ravioli",
    "red_velvet_cake",
    "risotto",
    "samosa",
    "sashimi",
    "scallops",
    "seaweed_salad",
    "shrimp_and_grits",
    "spaghetti_bolognese",
    "spaghetti_carbonara",
    "spring_rolls",
    "steak",
    "strawberry_shortcake",
    "sushi",
    "tacos",
    "takoyaki",
    "tiramisu",
    "tuna_tartare",
    "waffles",
]

TEXT_MAX_LENGTH = 50  # BERT tokenizerë¡œ ìë¥¼ ìµœëŒ€ ê¸¸ì´
MIN_FREQ = 3          # ì‚¬ìš©ì€ ì•ˆ ë¨ (ì•„ë§ˆ ë‹¤ë¥¸ ê³³ì—ì„œ ì“°ëŠ” global ìƒìˆ˜ë¡œ ì„ ì–¸ë§Œ í•´ë‘ )
NUMBER_OF_SAMPLES_PER_CLASS = None  # (í˜„ì¬ ì‚¬ìš© X)


# ğŸ”¹ 1. Datapool í´ë˜ìŠ¤ (ì „ì²´ ë°ì´í„°ì…‹ì˜ ê´€ë¦¬ êµ¬ì¡°)
class Datapool(Dataset):
    def __init__(self, all_ids, mode):
        self.all_ids = all_ids
        self.mode = mode

        # ID ì¤‘ë³µ ì²´í¬ (ì¤‘ë³µë˜ë©´ ì—ëŸ¬ ë°œìƒì‹œí‚´)
        assert len(list(set(self.all_ids))) == len(self.all_ids), "dataset has duplicated ids"

        # labeled / unlabeled split ì´ˆê¸°í™”
        self.unlabeled_ids = self.all_ids.copy()  # ì²˜ìŒì—” ì „ë¶€ unlabeled ìƒíƒœ
        self.labeled_ids = []

        # ê¸°ë³¸ sample_ids ì„¤ì • (test/valìš©ì¼ ê²½ìš° ì „ë¶€ ì‚¬ìš©)
        self.sample_ids = self.all_ids.copy()

    def initialize(self, query_budget: int):
        """
        Active learning ì´ˆê¸°ì— ë¼ë²¨ë§í•  ìƒ˜í”Œì„ randomìœ¼ë¡œ ì„ íƒí•˜ëŠ” í•¨ìˆ˜
        query_budgetë§Œí¼ ë½‘ê³  ë‚˜ë¨¸ì§€ë¥¼ unlabeledë¡œ ìœ ì§€
        """
        self.labeled_ids = self.unlabeled_ids[:query_budget]
        self.unlabeled_ids = [id for id in self.all_ids if id not in self.labeled_ids]

    def query_for_label(self, queried_ids: list):
        """
        Query ì „ëµì´ ì„ íƒí•œ ìƒ˜í”Œë“¤ì„ ë¼ë²¨ë§ëœ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        """
        self.labeled_ids += queried_ids
        self.unlabeled_ids = [id for id in self.all_ids if id not in self.labeled_ids]
        assert len(self.labeled_ids) + len(self.unlabeled_ids) == len(self.all_ids)

    def query(self):
        """
        ì¿¼ë¦¬ìš© ë°ì´í„°ì…‹ ì„¤ì • (ë¼ë²¨ë§ ì•ˆ ëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•¨)
        """
        self.mode = "query"
        print("dataset for querying")
        self.sample_ids = self.unlabeled_ids

    def train(self):
        """
        í•™ìŠµìš© ë°ì´í„°ì…‹ ì„¤ì • (ë¼ë²¨ë§ ëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•¨)
        """
        self.mode = "train"
        print("dataset for training")
        self.sample_ids = self.labeled_ids

    def __len__(self):
        return len(self.sample_ids)

    def __getitem__(self, idx):
        # ì¶”ìƒ í´ë˜ìŠ¤ë¼ì„œ êµ¬í˜„í•˜ì§€ ì•ŠìŒ. Food101ì—ì„œ overrideí•¨
        pass


# ğŸ”¹ 2. Food101 í´ë˜ìŠ¤ (ì‹¤ì œ food101 ë°ì´í„°ì…‹ì— ëŒ€í•œ êµ¬ì²´ êµ¬í˜„)
class Food101(Datapool):
    def __init__(
        self,
        mode="train",
        dataset_root_dir=r"data/food101/",
    ):
        self.dataset_root_dir = dataset_root_dir
        self.mode = mode
        assert self.mode in ["train", "dev", "test"]  # mode ìœ íš¨ì„± ì²´í¬

        # JSONì—ì„œ ë°ì´í„° ë¡œë“œ â†’ IDë³„ë¡œ dict êµ¬ì„±
        with open(join(dataset_root_dir, f"{mode}.json")) as file:
            data_list = json.load(file)
            self.data = {x["id"]: x for x in data_list}

        # ì „ì²´ ìƒ˜í”Œ id ëª©ë¡ ìƒì„± í›„ ì„ê¸°
        self.all_ids = list(self.data.keys())
        random.Random(0).shuffle(self.all_ids)  # seed ê³ ì •

        # ë¶€ëª¨ í´ë˜ìŠ¤(Datapool) ì´ˆê¸°í™”
        super(Food101, self).__init__(self.all_ids, self.mode)

        # Augmentation ì„¤ì •
        color_distort_strength = 0.5
        color_jitter = transforms.ColorJitter(
            brightness=0.8 * color_distort_strength,
            contrast=0.8 * color_distort_strength,
            saturation=0.8 * color_distort_strength,
            hue=0.2 * color_distort_strength,
        )
        gaussian_kernel_size = 21

        # í•™ìŠµìš© transform (augmentation í¬í•¨)
        self.train_transform = transforms.Compose([
            transforms.Resize([224, 224]),
            transforms.CenterCrop(224),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomApply([color_jitter], p=0.8),
            transforms.RandomGrayscale(p=0.2),
            transforms.GaussianBlur(kernel_size=gaussian_kernel_size),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
        ])

        # ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© transform (augmentation ì—†ìŒ)
        self.val_transform = transforms.Compose([
            transforms.Resize([224, 224]),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
        ])

        # í…ìŠ¤íŠ¸ ê´€ë ¨ ì„¤ì •
        self.sentence_max_len = TEXT_MAX_LENGTH
        self.tokenizer = BertTokenizer.from_pretrained("pre-trained BERT")  # ëª¨ë¸ ì´ë¦„ì€ ìˆ˜ì • í•„ìš”í•  ìˆ˜ë„ ìˆìŒ

    def load_bert_tokens(self, sample_id):
        """
        ì£¼ì–´ì§„ ìƒ˜í”Œì˜ í…ìŠ¤íŠ¸ tokenì„ BERT tokenizerë¡œ ì¸ì½”ë”©í•¨
        ë°˜í™˜ í˜•ì‹ì€ tensor dict (input_ids, attention_mask ë“±)
        """
        text_tokens = " ".join(self.data[sample_id]["text_tokens"])
        text_input = self.tokenizer(
            text_tokens,
            add_special_tokens=True,
            padding="max_length",
            max_length=self.sentence_max_len,
            truncation=True,
            return_tensors="pt",
        )
        # (1, L) â†’ (L,)ë¡œ squeeze
        for k, v in text_input.items():
            text_input[k] = v.squeeze(0)
        return text_input

    def load_image(self, sample_id):
        """
        ì´ë¯¸ì§€ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ê³ , ì ì ˆí•œ transform ì ìš©
        """
        image_path = join(self.dataset_root_dir, self.data[sample_id]["img_path"])
        with open(image_path, "rb") as f:
            image = Image.open(f).convert("RGB")
        if self.mode == "train":
            image = self.train_transform(image)
        else:
            image = self.val_transform(image)
        return image

    def __len__(self):
        return len(self.sample_ids)

    def __getitem__(self, idx):
        """
        í•˜ë‚˜ì˜ ìƒ˜í”Œì„ ë¶ˆëŸ¬ì™€ì„œ (text_input, image, label) ë°˜í™˜
        text_input: BERT input
        image: transformëœ ì´ë¯¸ì§€ tensor
        label: ì •ìˆ˜í˜• í´ë˜ìŠ¤ index (0~100)
        """
        sample_id = self.sample_ids[idx]
        text_input = self.load_bert_tokens(sample_id)
        class_name = self.data[sample_id]["label"]
        image = self.load_image(sample_id)
        label = torch.tensor(CLASS_NAME.index(class_name), dtype=torch.long)
        return text_input, image, label
