import torch
from torch import nn
import torch.nn.functional as F

from models.transformer import TransformerEncoder
from models.adapter import Adapter

# ------------------------------------------------------------------------
# Latefusion: ê° modalityë¥¼ Transformerë¡œ ì¸ì½”ë”© í›„ feature-levelì—ì„œ ìœµí•©
# ------------------------------------------------------------------------

class Latefusion(nn.Module):
    def __init__(
        self,
        orig_dim,        # ê° modalityì˜ ì…ë ¥ ì°¨ì› ë¦¬ìŠ¤íŠ¸ (ex. [text_dim, audio_dim, video_dim])
        output_dim=1,    # ì¶œë ¥ ì°¨ì› (íšŒê·€ë©´ 1, ë¶„ë¥˜ë©´ í´ë˜ìŠ¤ ìˆ˜)
        proj_dim=40,     # ê° modalityì˜ feature projection ì°¨ì›
        num_heads=5,
        layers=5,
        relu_dropout=0.1,
        embed_dropout=0.15,
        res_dropout=0.1,
        out_dropout=0.1,
        attn_dropout=0.2,
    ):
        super(Latefusion, self).__init__()

        self.proj_dim = proj_dim
        self.orig_dim = orig_dim
        self.num_mod = len(orig_dim)  # modality ìˆ˜ (ì˜ˆ: 3ê°œ)
        self.num_heads = num_heads
        self.layers = layers
        self.attn_dropout = attn_dropout
        self.relu_dropout = relu_dropout
        self.res_dropout = res_dropout
        self.out_dropout = out_dropout
        self.embed_dropout = embed_dropout

        # ğŸ”¹ 1. ê° modalityë³„ 1D Conv projection layer (dim ì •ë ¬ìš©)
        self.proj = nn.ModuleList([
            nn.Conv1d(self.orig_dim[i], self.proj_dim, kernel_size=1, padding=0)
            for i in range(self.num_mod)
        ])

        # ğŸ”¹ 2. ê° modalityë³„ ë…ë¦½ì ì¸ TransformerEncoder
        self.encoders = nn.ModuleList([
            TransformerEncoder(
                embed_dim=proj_dim,
                num_heads=self.num_heads,
                layers=self.layers,
                attn_dropout=self.attn_dropout,
                res_dropout=self.res_dropout,
                relu_dropout=self.relu_dropout,
                embed_dropout=self.embed_dropout,
            )
            for _ in range(self.num_mod)
        ])

        # ğŸ”¹ 3. ì¶œë ¥ (ë¶„ë¥˜/íšŒê·€ìš©) MLP í—¤ë“œ
        self.out_layer_proj0 = nn.Linear(3 * self.proj_dim, self.proj_dim)
        self.out_layer_proj1 = nn.Linear(self.proj_dim, self.proj_dim)
        self.out_layer_proj2 = nn.Linear(self.proj_dim, self.proj_dim)
        self.out_layer = nn.Linear(self.proj_dim, output_dim)

    def get_emb(self, x):
        """
        ì…ë ¥ modalityë“¤ì„ Transformerë¡œ ê°ê° ì¸ì½”ë”©í•˜ì—¬ ë°˜í™˜
        x: modalityë³„ ë¦¬ìŠ¤íŠ¸, ê° í…ì„œ (B, T, D_i)
        ë°˜í™˜ê°’: ë¦¬ìŠ¤íŠ¸ [h1, h2, h3], ê° h_iëŠ” (B, T, proj_dim)
        """
        hs = list()
        for i in range(self.num_mod):
            x[i] = x[i].transpose(1, 2)               # (B, D, T)
            x[i] = self.proj[i](x[i])                 # Conv1D â†’ (B, proj_dim, T)
            x[i] = x[i].permute(2, 0, 1)              # (T, B, proj_dim)
            h_tmp = self.encoders[i](x[i]).permute(1, 0, 2)  # (B, T, proj_dim)
            hs.append(h_tmp)
        return hs

    def get_res(self, x):
        """
        classification/regression outputì„ ìƒì„±í•˜ëŠ” í—¤ë“œ ë¶€ë¶„
        x: (B, 3 * proj_dim)
        """
        last_hs = F.relu(self.out_layer_proj0(x))  # 1ì°¨ íˆ¬ì˜
        last_hs_proj = self.out_layer_proj2(
            F.dropout(
                F.relu(self.out_layer_proj1(last_hs)),  # 2ì°¨ íˆ¬ì˜
                p=self.out_dropout,
                training=self.training,
            )
        )
        last_hs_proj += last_hs  # residual ì—°ê²°
        output = self.out_layer(last_hs_proj)
        return output

    def forward(self, x):
        """
        ì „ì²´ forward íë¦„:
        - ê° modality â†’ ì¸ì½”ë”©
        - ì¸ì½”ë”©ëœ ê²°ê³¼ â†’ ì²« íƒ€ì„ìŠ¤í…ë§Œ ë½‘ì•„ì„œ concat
        - MLP í†µê³¼ í›„ output ë°˜í™˜
        """
        hs = list()
        for i in range(self.num_mod):
            x[i] = x[i].transpose(1, 2)
            x[i] = self.proj[i](x[i])
            x[i] = x[i].permute(2, 0, 1)
            h_tmp = self.encoders[i](x[i])
            hs.append(h_tmp[0])  # ì²« timestepë§Œ ì‚¬ìš© (CLSì²˜ëŸ¼)

        last_hs_out = torch.cat(hs, dim=-1)  # (B, 3 * proj_dim)
        last_hs = F.relu(self.out_layer_proj0(last_hs_out))
        last_hs_proj = self.out_layer_proj2(
            F.dropout(
                F.relu(self.out_layer_proj1(last_hs)),
                p=self.out_dropout,
                training=self.training,
            )
        )
        last_hs_proj += last_hs
        output = self.out_layer(last_hs_proj)
        return output

# ------------------------------------------------------------------------
# AdapterModel: Latefusion + Wander Adapter
# ------------------------------------------------------------------------

class AdapterModel(nn.Module):
    def __init__(
        self,
        orig_dim,         # ê° modality ì…ë ¥ ì°¨ì›
        t_dim,            # ê° modalityì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ (time_dim)
        rank, drank, trank,  # adapter ê´€ë ¨ ì°¨ì› ì„¤ì •
        output_dim=1,
        proj_dim=40,
        num_heads=5,
        layers=5,
        relu_dropout=0.1,
        embed_dropout=0.15,
        res_dropout=0.1,
        out_dropout=0.1,
        attn_dropout=0.2,
    ):
        super().__init__()
        self.num_mod = len(orig_dim)

        # ğŸ”¹ base ëª¨ë¸: Latefusion êµ¬ì¡° (Transformer + MLP)
        self.basemodel = Latefusion(
            orig_dim,
            output_dim,
            proj_dim,
            num_heads,
            layers,
            relu_dropout,
            embed_dropout,
            res_dropout,
            out_dropout,
            attn_dropout,
        )

        # ğŸ”¹ Wander Adapter ì¶”ê°€ (ë©€í‹°ëª¨ë‹¬ ìœµí•© ë‹´ë‹¹)
        self.adapter = Adapter(self.num_mod, proj_dim, t_dim, rank, drank, trank)

        # ğŸ”¹ base model freeze
        self.basemodel.requires_grad_(False)

        # ğŸ”¹ prediction headë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        self.basemodel.out_layer_proj0.requires_grad_(True)
        self.basemodel.out_layer_proj1.requires_grad_(True)
        self.basemodel.out_layer_proj2.requires_grad_(True)
        self.basemodel.out_layer.requires_grad_(True)

    def forward(self, x):
        """
        1. ê° modalityë¥¼ Transformerë¡œ ì¸ì½”ë”©
        2. Adapterë¡œ ë©€í‹°ëª¨ë‹¬ ìœµí•©
        3. ìœµí•©ëœ featureë¡œ ìµœì¢… prediction ìˆ˜í–‰
        """
        hs = self.basemodel.get_emb(x)          # â†’ ê° modalityë³„ ì¸ì½”ë”© ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        fusion = self.adapter(hs)               # â†’ Wander adapter ìœµí•© (B, T, D)

        # Adapter ì¶œë ¥: (B, D, T)ë¡œ ë³€ê²½ í›„, ê° modalityì˜ [T=0] ê°’ë§Œ ëª¨ìŒ
        fusion = fusion.permute(0, 2, 1)        # (B, D, T)
        fusion = torch.cat([fusion[:, :, 0] for _ in range(self.num_mod)], dim=-1)  # (B, 3*D)

        output = self.basemodel.get_res(fusion)
        return output
