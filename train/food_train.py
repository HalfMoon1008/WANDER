import torch
from torch import nn
import torch.optim as optim
import time
from torch.optim.lr_scheduler import ReduceLROnPlateau

from models.foodmodel import FoodModelWander
from utils.eval_metrics import eval_food
from utils.util import transfer_model, get_parameter_number

# ---------------------------------------------------------------
# ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ ì „ì²´ ìˆ˜í–‰ (entry point)
# ---------------------------------------------------------------

def initiate(hyp_params, train_loader, valid_loader, test_loader):
    """
    ì „ì²´ í•™ìŠµ ê³¼ì •ì˜ ì§„ìž…ì 
    - ëª¨ë¸ ì´ˆê¸°í™”
    - ì‚¬ì „í•™ìŠµ weight ë¡œë“œ
    - optimizer, scheduler, loss ì„¸íŒ…
    - train_model í˜¸ì¶œ
    """
    model = FoodModelWander(
        hyp_params.pretrained_vit,
        hyp_params.pretrained_text,
        hyp_params.output_dim,
        hyp_params.t_dim,
        hyp_params.rank,
        hyp_params.drank,
        hyp_params.trank,
        hyp_params.out_dropout
    )

    # ðŸ”¹ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ weight ë¶ˆëŸ¬ì˜¤ê¸°
    transfer_model(hyp_params.pretrained_model, model)

    # ðŸ”¹ ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    print(get_parameter_number(model))

    if hyp_params.use_cuda:
        model = model.cuda()

    # ðŸ”¹ optimizer ìƒì„± (ex. Adam, SGD ë“± ë¬¸ìžì—´ë¡œ ë°›ìŒ)
    optimizer = getattr(optim, hyp_params.optim)(
        model.parameters(), lr=hyp_params.lr, weight_decay=4e-5
    )

    # ðŸ”¹ loss í•¨ìˆ˜ ì„¤ì • (ex. CrossEntropyLoss ë“±)
    criterion = getattr(nn, hyp_params.criterion)()

    # ðŸ”¹ learning rate scheduler ì„¤ì •
    scheduler = ReduceLROnPlateau(
        optimizer, mode="min", patience=hyp_params.when, factor=0.5, verbose=True
    )

    settings = {
        "model": model,
        "optimizer": optimizer,
        "criterion": criterion,
        "scheduler": scheduler,
    }

    return train_model(settings, hyp_params, train_loader, valid_loader, test_loader)

# ---------------------------------------------------------------
# ì „ì²´ í•™ìŠµ ë£¨í”„
# ---------------------------------------------------------------

def train_model(settings, hyp_params, train_loader, valid_loader, test_loader):
    model = settings["model"]
    optimizer = settings["optimizer"]
    criterion = settings["criterion"]
    scheduler = settings["scheduler"]

    # ðŸ”¹ í•™ìŠµ ë£¨í‹´ ì •ì˜
    def train(model, optimizer, criterion):
        model.train()
        for i_batch, batch in enumerate(train_loader):
            text, image, batch_Y = batch
            eval_attr = batch_Y.squeeze(-1)

            model.zero_grad()

            if hyp_params.use_cuda:
                with torch.cuda.device(0):
                    # í…ìŠ¤íŠ¸ BERT ìž…ë ¥ ë¶„ë¦¬
                    ti, ta, tt = (
                        text["input_ids"].cuda(),
                        text["attention_mask"].cuda(),
                        text["token_type_ids"].cuda(),
                    )
                    image, eval_attr = image.cuda(), eval_attr.cuda()
                    eval_attr = eval_attr.long()

            batch_size = image.size(0)

            # ðŸ”¸ í° ë°°ì¹˜ë©´ DataParallel ì‚¬ìš©
            net = nn.DataParallel(model) if batch_size > 10 else model

            # ðŸ”¸ ëª¨ë¸ forward
            preds = net(image, [ti, ta, tt])  # shape: (B, 101)
            preds = preds.view(-1, 101)
            eval_attr = eval_attr.view(-1)

            # ðŸ”¸ ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ
            raw_loss = criterion(preds, eval_attr)
            raw_loss.backward()

            # ðŸ”¸ gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), hyp_params.clip)

            optimizer.step()

    # ðŸ”¹ í‰ê°€ ë£¨í‹´ ì •ì˜ (ê²€ì¦ or í…ŒìŠ¤íŠ¸)
    def evaluate(model, criterion, test=False):
        model.eval()
        loader = test_loader if test else valid_loader
        total_loss = 0.0
        results = []
        truths = []

        with torch.no_grad():
            for i_batch, batch in enumerate(loader):
                text, image, batch_Y = batch
                eval_attr = batch_Y.squeeze(dim=-1)

                if hyp_params.use_cuda:
                    with torch.cuda.device(0):
                        ti, ta, tt = (
                            text["input_ids"].cuda(),
                            text["attention_mask"].cuda(),
                            text["token_type_ids"].cuda(),
                        )
                        image, eval_attr = image.cuda(), eval_attr.cuda()
                        eval_attr = eval_attr.long()

                preds = model(image, [ti, ta, tt])
                preds = preds.view(-1, 101)
                eval_attr = eval_attr.view(-1)
                total_loss += criterion(preds, eval_attr).item()

                results.append(preds)
                truths.append(eval_attr)

        avg_loss = total_loss / (hyp_params.n_test if test else hyp_params.n_valid)
        results = torch.cat(results)
        truths = torch.cat(truths)
        return avg_loss, results, truths

    # -----------------------------------------------------------
    # ðŸ”¹ ë³¸ê²©ì ì¸ í•™ìŠµ ë°˜ë³µ
    # -----------------------------------------------------------

    best_acc = 0
    for epoch in range(1, hyp_params.num_epochs + 1):
        start = time.time()

        train(model, optimizer, criterion)
        val_loss, r, t = evaluate(model, criterion, test=False)
        acc = eval_food(r, t)  # ðŸ”¸ í‰ê°€ ì§€í‘œ (F1, acc ë“± í¬í•¨)

        end = time.time()
        duration = end - start

        # ðŸ”¹ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ learning rate decay
        scheduler.step(val_loss)

        print("-" * 50)
        print(f"Epoch {epoch:2d} | Time {duration:.4f} sec | Valid Loss {val_loss:.4f}")
        print("-" * 50)

        if acc > best_acc:
            best_acc = acc

    print("Best accuracy of validation: {:.4f}".format(best_acc))
